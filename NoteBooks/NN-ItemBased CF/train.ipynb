{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4119eb0b",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK HYBRID COLLOBORATIVE FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994ee7b",
   "metadata": {},
   "source": [
    "The model is learning latent factors for both users and items using a neural network. The similarity function computes the similarity between two items based on their latent factors, which is related to item-based collaborative filtering. The rating function predicts a user's rating for an item based on the user's latent factors and the item's latent factors, which combines both user-based and item-based collaborative filtering.\n",
    "\n",
    "Thus, the provided code is implementing a hybrid collaborative filtering approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44acbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and process data\n",
    "df_anime = pd.read_csv('animes.csv')\n",
    "df_reviews = pd.read_csv('reviews.csv')\n",
    "df_anime = df_anime[['uid', 'title']]\n",
    "df_anime.rename(columns={'uid': 'anime_id'}, inplace=True)\n",
    "df_anime = df_anime.drop_duplicates()\n",
    "\n",
    "df_reviews_cp = df_reviews[['profile', 'anime_uid', 'score']]\n",
    "df_reviews_cp = df_reviews_cp[df_reviews_cp['score'] != -1]\n",
    "df_reviews_cp['score'] = df_reviews_cp['score'] / 10  # Scale the scores to the range of 0 to 5\n",
    "df_reviews_cp = df_reviews_cp.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Map user and anime ids to integer indices\n",
    "user_mapping = {user_id: idx for idx, user_id in enumerate(df_reviews_cp.profile.unique())}\n",
    "anime_mapping = {anime_id: idx for idx, anime_id in enumerate(df_reviews_cp.anime_uid.unique())}\n",
    "\n",
    "\n",
    "df_reviews_cp['user_id'] = df_reviews_cp['profile'].apply(lambda x: user_mapping[x])\n",
    "df_reviews_cp['anime_id'] = df_reviews_cp['anime_uid'].apply(lambda x: anime_mapping[x])\n",
    "\n",
    "# Define neural network-based item similarity and rating prediction model\n",
    "class ItemSimilarityAndRating(nn.Module):\n",
    "    def __init__(self, n_users, n_anime, n_factors):\n",
    "        super(ItemSimilarityAndRating, self).__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.anime_factors = nn.Embedding(n_anime, n_factors)\n",
    "\n",
    "    def similarity(self, anime1, anime2):\n",
    "        dot_product = (self.anime_factors(anime1) * self.anime_factors(anime2)).sum(1)\n",
    "        return dot_product\n",
    "\n",
    "    def rating(self, user, anime):\n",
    "        dot_product = (self.user_factors(user) * self.anime_factors(anime)).sum(1)\n",
    "        return torch.sigmoid(dot_product) * 10  # Scale the predicted ratings to the range of 0 to 5\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "n_users = len(user_mapping)\n",
    "n_anime = len(anime_mapping)\n",
    "n_factors = 200\n",
    "\n",
    "model = ItemSimilarityAndRating(n_users, n_anime, n_factors)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 25\n",
    "batch_size = 1024\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_data, val_data = train_test_split(df_reviews_cp, test_size=0.2, random_state=42)\n",
    "    shuffled_indices = torch.randperm(len(train_data))\n",
    "    for batch_start in range(0, len(train_data), batch_size):\n",
    "        batch_indices = shuffled_indices[batch_start:batch_start + batch_size]\n",
    "        user_batch = torch.tensor(train_data.iloc[batch_indices]['user_id'].values, dtype=torch.long)\n",
    "        anime_batch = torch.tensor(train_data.iloc[batch_indices]['anime_id'].values, dtype=torch.long)\n",
    "        score_batch = torch.tensor(train_data.iloc[batch_indices]['score'].values, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model.rating(user_batch, anime_batch)\n",
    "        loss = loss_func(predictions, score_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee6597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and process data\n",
    "df_anime = pd.read_csv('animes.csv')\n",
    "df_reviews = pd.read_csv('reviews.csv')\n",
    "df_anime = df_anime[['uid', 'title','genre']]\n",
    "df_anime.rename(columns={'uid': 'anime_id'}, inplace=True)\n",
    "df_anime = df_anime.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data(username, anime_uid=0, score, df_reviews=''):\n",
    "    # Create a new DataFrame with the data to append\n",
    "    new_data = pd.DataFrame({'profile': [username], 'anime_uid': [anime_uid], 'score': [score]})\n",
    "\n",
    "    # Append the new data to the existing DataFrame\n",
    "    df_reviews = df_reviews.append(new_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764fd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_18_above = ['Hentai', 'Ecchi', 'Harem', 'Yuri', 'Yaoi']\n",
    "\n",
    "def is_18_above(genre_str):\n",
    "    if isinstance(genre_str, str):\n",
    "        for genre in genres_18_above:\n",
    "            if genre in genre_str:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "df_anime['18_above'] = df_anime['genre'].apply(is_18_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d52d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime['18_above'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0380ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime = df_anime[df_anime['18_above'] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63780958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and process data\n",
    "df_anime = pd.read_csv('animes.csv')\n",
    "df_reviews = pd.read_csv('reviews.csv')\n",
    "df_anime = df_anime[['uid', 'title', 'genre']]\n",
    "df_anime.rename(columns={'uid': 'anime_id'}, inplace=True)\n",
    "df_anime = df_anime.drop_duplicates()\n",
    "\n",
    "df_reviews_cp = df_reviews[['profile', 'anime_uid', 'score']]\n",
    "df_reviews_cp = df_reviews_cp[df_reviews_cp['score'] != -1]\n",
    "df_reviews_cp['score'] = df_reviews_cp['score'] / 10  # Scale the scores to the range of 0 to 5\n",
    "df_reviews_cp = df_reviews_cp.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Map user and anime ids to integer indices\n",
    "user_mapping = {user_id: idx for idx, user_id in enumerate(df_reviews_cp.profile.unique())}\n",
    "anime_mapping = {anime_id: idx for idx, anime_id in enumerate(df_reviews_cp.anime_uid.unique())}\n",
    "\n",
    "\n",
    "df_reviews_cp['user_id'] = df_reviews_cp['profile'].apply(lambda x: user_mapping[x])\n",
    "df_reviews_cp['anime_id'] = df_reviews_cp['anime_uid'].apply(lambda x: anime_mapping[x])\n",
    "\n",
    "genres_18_above = ['Hentai', 'Ecchi', 'Harem', 'Yuri', 'Yaoi']\n",
    "\n",
    "def is_18_above(genre_str):\n",
    "    if isinstance(genre_str, str):\n",
    "        for genre in genres_18_above:\n",
    "            if genre in genre_str:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "df_anime['18_above'] = df_anime['genre'].apply(is_18_above)\n",
    "\n",
    "# Define neural network-based item similarity and rating prediction model\n",
    "class ItemSimilarityAndRating(nn.Module):\n",
    "    def __init__(self, n_users, n_anime, n_factors):\n",
    "        super(ItemSimilarityAndRating, self).__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.anime_factors = nn.Embedding(n_anime, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)  # Add user bias\n",
    "        self.anime_bias = nn.Embedding(n_anime, 1)  # Add anime bias\n",
    "\n",
    "    def rating(self, user, anime):\n",
    "        dot_product = (self.user_factors(user) * self.anime_factors(anime)).sum(1)\n",
    "        rating = dot_product + self.user_bias(user).squeeze() + self.anime_bias(anime).squeeze()\n",
    "        return torch.sigmoid(rating) * 10\n",
    "\n",
    "\n",
    "    def similarity(self, anime1, anime2):\n",
    "        dot_product = (self.anime_factors(anime1) * self.anime_factors(anime2)).sum(1)\n",
    "        return dot_product\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "n_users = len(user_mapping)\n",
    "n_anime = len(anime_mapping)\n",
    "n_factors = 200\n",
    "\n",
    "model = ItemSimilarityAndRating(n_users, n_anime, n_factors)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "batch_size = 1024\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_data, val_data = train_test_split(df_reviews_cp, test_size=0.2, random_state=42)\n",
    "    shuffled_indices = torch.randperm(len(train_data))\n",
    "    for batch_start in range(0, len(train_data), batch_size):\n",
    "        batch_indices = shuffled_indices[batch_start:batch_start + batch_size]\n",
    "        user_batch = torch.tensor(train_data.iloc[batch_indices]['user_id'].values, dtype=torch.long)\n",
    "        anime_batch = torch.tensor(train_data.iloc[batch_indices]['anime_id'].values, dtype=torch.long)\n",
    "        score_batch = torch.tensor(train_data.iloc[batch_indices]['score'].values, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model.rating(user_batch, anime_batch)\n",
    "        loss = loss_func(predictions, score_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd092a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "val_user_batch = torch.tensor(val_data['user_id'].values, dtype=torch.long)\n",
    "val_anime_batch = torch.tensor(val_data['anime_id'].values, dtype=torch.long)\n",
    "val_score_batch = torch.tensor(val_data['score'].values, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_predictions = model.rating(val_user_batch, val_anime_batch)\n",
    "    val_loss = loss_func(val_predictions, val_score_batch)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38399b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions_np = val_predictions.numpy()\n",
    "result_df = pd.DataFrame({\n",
    "    'user_id': val_data['user_id'].values,\n",
    "    'anime_id': val_data['anime_id'].values,\n",
    "    'original_score': val_data['score'].values*10,\n",
    "    'predicted_score': np.round(val_predictions_np,0)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = torch.mean(torch.abs(val_predictions - val_score_batch)).item()\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'item_similarity_and_rating_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = ItemSimilarityAndRating(n_users, n_anime, n_factors)\n",
    "loaded_model.load_state_dict(torch.load('item_similarity_and_rating_model.pth'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6eb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 similar animes\n",
    "anime_id = 1  # Change this to the ID of the anime you want to find similar animes for\n",
    "anime_index = anime_mapping[anime_id]\n",
    "anime_tensor = torch.tensor([anime_index] * n_anime, dtype=torch.long)\n",
    "other_anime_tensor = torch.tensor(list(range(n_anime)), dtype=torch.long)\n",
    "\n",
    "anime_similarities = loaded_model.similarity(anime_tensor, other_anime_tensor)\n",
    "\n",
    "# Get top 11 similar animes\n",
    "top_11_indices = torch.topk(anime_similarities, 11).indices\n",
    "\n",
    "top_10_indices = top_11_indices[1:]\n",
    "\n",
    "top_10_anime_ids = [list(anime_mapping.keys())[list(anime_mapping.values()).index(idx)] for idx in top_10_indices.tolist()]\n",
    "recommended_anime = df_anime[df_anime['anime_id'].isin(top_10_anime_ids)].reset_index(drop=True)\n",
    "# Filter out animes with 18_above == 1\n",
    "recommended_anime = recommended_anime[recommended_anime['18_above'] == 0].reset_index(drop=True)\n",
    "\n",
    "top_10_similarities = anime_similarities[top_10_indices].tolist()\n",
    "print(\"Top 10 Similar Animes and Similarity Scores:\")\n",
    "for idx, row in recommended_anime.iterrows():\n",
    "    print(f\"{row['title']} (Similarity Score: {top_10_similarities[idx]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 similar animes\n",
    "anime_id = 1  # Change this to the ID of the anime you want to find similar animes for\n",
    "anime_index = anime_mapping[anime_id]\n",
    "anime_tensor = torch.tensor([anime_index] * n_anime, dtype=torch.long)\n",
    "other_anime_tensor = torch.tensor(list(range(n_anime)), dtype=torch.long)\n",
    "\n",
    "anime_similarities = loaded_model.similarity(anime_tensor, other_anime_tensor)\n",
    "\n",
    "# Get top 11 similar animes\n",
    "top_11_indices = torch.topk(anime_similarities, 11).indices\n",
    "\n",
    "top_10_indices = top_11_indices[1:]\n",
    "\n",
    "top_10_anime_ids = [list(anime_mapping.keys())[list(anime_mapping.values()).index(idx)] for idx in top_10_indices.tolist()]\n",
    "recommended_anime = df_anime[df_anime['anime_id'].isin(top_10_anime_ids)].reset_index(drop=True)\n",
    "# Filter out animes with 18_above == 1\n",
    "recommended_anime = recommended_anime[recommended_anime['18_above'] == 0].reset_index(drop=True)\n",
    "\n",
    "top_10_similarities = anime_similarities[top_10_indices].tolist()\n",
    "print(\"Top 10 Similar Animes and Similarity Scores:\")\n",
    "for idx, row in recommended_anime.iterrows():\n",
    "    print(f\"{row['title']} (Similarity Score: {top_10_similarities[idx]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f33313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings for a user\n",
    "user_id = 'baekbeans'  # Change this to the user ID for which you want to predict ratings\n",
    "user_index = user_mapping[user_id]\n",
    "user_tensor = torch.tensor([user_index] * n_anime, dtype=torch.long)\n",
    "\n",
    "predicted_ratings = loaded_model.rating(user_tensor, other_anime_tensor)\n",
    "\n",
    "unique_anime_ids = df_reviews_cp['anime_uid'].unique()\n",
    "unique_animes_df = df_anime[df_anime['anime_id'].isin(unique_anime_ids)].reset_index(drop=True)\n",
    "predicted_ratings_df = pd.DataFrame({'title': unique_animes_df['title'], 'predicted_rating': predicted_ratings.tolist()})\n",
    "\n",
    "predicted_ratings_df = predicted_ratings_df.sort_values(by='predicted_rating', ascending=False).head(10)\n",
    "print(\"\\nPredicted Ratings for User:\")\n",
    "print(predicted_ratings_df['predicted_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map all anime ids to indices\n",
    "unique_anime_ids = df_reviews_cp['anime_uid'].unique()\n",
    "unique_animes_df = df_anime[df_anime['anime_id'].isin(unique_anime_ids)].reset_index(drop=True)\n",
    "n_unique_anime = len(unique_anime_ids)\n",
    "\n",
    "# Create user_tensor and other_anime_tensor for unique animes\n",
    "user_tensor_unique = torch.tensor([user_index] * n_unique_anime, dtype=torch.long)\n",
    "other_anime_tensor_unique = torch.tensor(list(range(n_unique_anime)), dtype=torch.long)\n",
    "\n",
    "# Predict ratings for unique animes\n",
    "predicted_ratings_unique = loaded_model.rating(user_tensor_unique, other_anime_tensor_unique)\n",
    "\n",
    "# Create a dataframe with predicted ratings for unique animes\n",
    "predicted_ratings_df = pd.DataFrame({'title': unique_animes_df['title'], 'predicted_rating': predicted_ratings_unique.tolist()})\n",
    "predicted_ratings_df['predicted_rating'] = predicted_ratings_df['predicted_rating'].apply(round)\n",
    "\n",
    "print(\"\\nPredicted Ratings for:\",user_id)\n",
    "print(predicted_ratings_df['predicted_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c7812",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings_df['predicted_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map all anime ids to indices\n",
    "unique_anime_ids = df_reviews_cp['anime_uid'].unique()\n",
    "unique_animes_df = df_anime[df_anime['anime_id'].isin(unique_anime_ids)].reset_index(drop=True)\n",
    "n_unique_anime = len(unique_anime_ids)\n",
    "\n",
    "# Create user_tensor and other_anime_tensor for unique animes\n",
    "user_tensor_unique = torch.tensor([user_index] * n_unique_anime, dtype=torch.long)\n",
    "other_anime_tensor_unique = torch.tensor(list(range(n_unique_anime)), dtype=torch.long)\n",
    "\n",
    "# Predict ratings for unique animes\n",
    "predicted_ratings_unique = model.rating(user_tensor_unique, other_anime_tensor_unique)\n",
    "\n",
    "# Create a dataframe with predicted ratings for unique animes\n",
    "predicted_ratings_df = pd.DataFrame({'title': unique_animes_df['title'], 'predicted_rating': predicted_ratings_unique.tolist()})\n",
    "predicted_ratings_df['predicted_rating'] = predicted_ratings_df['predicted_rating'].apply(round)\n",
    "\n",
    "print(\"\\nPredicted Ratings for User:\")\n",
    "print(predicted_ratings_df['predicted_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3675bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings_df['predicted_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e865553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
